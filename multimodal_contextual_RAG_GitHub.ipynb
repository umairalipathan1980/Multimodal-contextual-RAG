{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1a680f94-448a-4d84-89ee-dcd9a7b1919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the parsed directory... AI_report_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parsed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 22:54:13,885 - INFO - Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from existing files.\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Multimodal, contextual RAG with BM25+re-ranking\"\"\"\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.core.schema import NodeWithScore, MetadataMode, QueryBundle\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.schema import ImageNode\n",
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from IPython.display import display, Markdown\n",
    "import cohere  \n",
    "import time   \n",
    "from rank_bm25 import BM25Okapi  \n",
    "import logging  \n",
    "from pydantic import PrivateAttr\n",
    "import nest_asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from llama_parse import LlamaParse\n",
    "import re\n",
    "from pathlib import Path\n",
    "import typing as t\n",
    "from typing import Any, List, Optional, Tuple\n",
    "from llama_index.core.schema import TextNode\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate\n",
    "from pydantic import PrivateAttr\n",
    "from llama_index.core import Settings\n",
    "import pickle\n",
    "\n",
    "load_dotenv()\n",
    "best_match_25 = 1\n",
    "re_ranking = 1\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    Settings,\n",
    ")\n",
    "nest_asyncio.apply()\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize Cohere client \n",
    "cohere_client = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "Settings.chunk_size = 800\n",
    "Settings.chunk_overlap = 400\n",
    "\n",
    "# Initialize LlamaParse wit vendor multimodal model\n",
    "# parser = LlamaParse(\n",
    "#     result_type=\"markdown\",\n",
    "#     parsing_instruction=\"You are given a document. Extract the text, tables, and images. Describe the images and summarize the tables in the overall context\",\n",
    "#     use_vendor_multimodal_model=True,\n",
    "#     vendor_multimodal_model_name=\"openai-gpt-4o-mini\",\n",
    "#     vendor_multimodal_api_key=openai.api_key,\n",
    "#     show_progress=True,\n",
    "#     verbose=True,\n",
    "#     invalidate_cache=True,\n",
    "#     do_not_cache=True,\n",
    "#     num_workers=8,\n",
    "#     language=\"en\",\n",
    "#     premium_mode=True,\n",
    "#     api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\")  # Replace with your LLAMA_CLOUD_API_KEY\n",
    "# )\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    premium_mode=True,\n",
    "    api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    ")\n",
    "\n",
    "# Define data directory\n",
    "DATA_DIR = \"files2\" #directory to read files from\n",
    "DATA_DIR_2 = \"parsed\" #directory to save parsing results\n",
    "\n",
    "# Function to read all files from a specified directory\n",
    "def read_docs(data_dir) -> List[str]:\n",
    "    files = []\n",
    "    for f in os.listdir(data_dir):\n",
    "        fname = os.path.join(data_dir, f)\n",
    "        if os.path.isfile(fname):\n",
    "            files.append(fname)\n",
    "    return files\n",
    "\n",
    "# Function to get page number of images using regex on file names\n",
    "def get_img_page_number(file_name):\n",
    "    match = re.search(r\"-page-(\\d+)\\.jpg$\", str(file_name))\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0\n",
    "\n",
    "# Function to get image files sorted by page\n",
    "def _get_sorted_image_files(image_dir):\n",
    "    raw_files = [f for f in list(Path(image_dir).iterdir()) if f.is_file()]\n",
    "    sorted_files = sorted(raw_files, key=get_img_page_number)\n",
    "    return sorted_files\n",
    "\n",
    "# Context prompt template for contextual chunking\n",
    "CONTEXT_PROMPT_TMPL = \"\"\"\n",
    "You are an AI assistant specializing in document analysis. Your task is to provide brief, relevant context for a chunk of text from the given document.\n",
    "Here is the document:\n",
    "<document>\n",
    "{document}\n",
    "</document>\n",
    "\n",
    "Here is the chunk we want to situate within the whole document:\n",
    "<chunk>\n",
    "{chunk}\n",
    "</chunk>\n",
    "\n",
    "Provide a concise context (2-3 sentences) for this chunk, considering the following guidelines:\n",
    "1. Identify the main topic or concept discussed in the chunk.\n",
    "2. Mention any relevant information or comparisons from the broader document context.\n",
    "3. If applicable, note how this information relates to the overall theme or purpose of the document.\n",
    "4. Include any key figures, dates, or percentages that provide important context.\n",
    "5. Do not use phrases like \"This chunk discusses\" or \"This section provides\". Instead, directly state the context.\n",
    "\n",
    "Please give a short succinct context to situate this chunk within the overall document to improve search retrieval of the chunk. \n",
    "Answer only with the succinct context and nothing else.\n",
    "\n",
    "Context:\n",
    "\"\"\"\n",
    "\n",
    "CONTEXT_PROMPT = PromptTemplate(CONTEXT_PROMPT_TMPL)\n",
    "\n",
    "# Function to generate context for each chunk\n",
    "def _assign_context(document: str, chunk: str, llm) -> str:\n",
    "    prompt = CONTEXT_PROMPT.format(document=document, chunk=chunk)\n",
    "    response = llm.complete(prompt)\n",
    "    context = response.text.strip()\n",
    "    return context\n",
    "\n",
    "# Function to create text nodes with context\n",
    "def retrieve_nodes(json_results, image_dir, llm) -> List[TextNode]:\n",
    "    nodes = []\n",
    "    for result in json_results:\n",
    "        json_dicts = result[\"pages\"]\n",
    "        document_name = result[\"file_path\"].split('/')[-1]\n",
    "        docs = [doc[\"md\"] for doc in json_dicts]  # Extract text\n",
    "        image_files = _get_sorted_image_files(image_dir)  # Extract images\n",
    "        # Join all docs to create the full document text\n",
    "        document_text = \"\\n\\n\".join(docs)\n",
    "        for idx, doc in enumerate(docs):\n",
    "            # Generate context for each chunk (page)\n",
    "            context = _assign_context(document_text, doc, llm)\n",
    "            # Combine context with the original chunk\n",
    "            contextualized_content = f\"{context}\\n\\n{doc}\"\n",
    "            # Create the text node with the contextualized content\n",
    "            chunk_metadata = {\"page_num\": idx + 1}\n",
    "            chunk_metadata[\"image_path\"] = str(image_files[idx])\n",
    "            chunk_metadata[\"parsed_text_markdown\"] = docs[idx]\n",
    "        \n",
    "            node = TextNode(\n",
    "                text=contextualized_content,\n",
    "                metadata=chunk_metadata,\n",
    "            )\n",
    "            nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0, max_tokens=1024)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Set LlamaIndex settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Set the PARSING variable\n",
    "PARSING = 0  # Set to 1 to perform parsing, 0 to load existing data\n",
    "\n",
    "# Main code block\n",
    "if PARSING == 1: ##If parsing of a new document is done\n",
    "    files = read_docs(data_dir = DATA_DIR)\n",
    "    dir_name = input(\"Write the name of the directory where parsing results will be stored\")\n",
    "    output_dir = os.path.join(DATA_DIR_2, dir_name)  # Define the new directory path\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)  # Create the directory\n",
    "    image_dir = output_dir + \"/images\"\n",
    "    os.makedirs(image_dir)\n",
    "    print(\"Parsing...\")\n",
    "    json_results = parser.get_json_result(files)\n",
    "    print(\"Getting image dictionaries...\")\n",
    "    images = parser.get_images(json_results, download_path=image_dir)\n",
    "    print(\"Retrieving nodes...\")\n",
    "    # Get text nodes\n",
    "    text_node_with_context = retrieve_nodes(json_results, image_dir, llm)\n",
    "    # Create the vector store index\n",
    "    index = VectorStoreIndex(text_node_with_context, embed_model=embed_model)\n",
    "    index.storage_context.persist(persist_dir=output_dir)\n",
    "    # Build BM25 index\n",
    "    documents = [node.text for node in text_node_with_context]\n",
    "    tokenized_documents = [doc.split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "    # Save bm25 and text_node_with_context\n",
    "    with open(os.path.join(output_dir, 'tokenized_documents.pkl'), 'wb') as f:\n",
    "        pickle.dump(tokenized_documents, f)\n",
    "    with open(os.path.join(output_dir, 'text_node_with_context.pkl'), 'wb') as f:\n",
    "        pickle.dump(text_node_with_context, f)\n",
    "    print(\"Parsing and indexing completed.\")\n",
    "else: ##if the parsed files are to be loaded\n",
    "    input_dir = input(\"Enter the name of the parsed directory...\")\n",
    "    input_dir = DATA_DIR_2 + \"/\"+ input_dir\n",
    "    print(\"Loading parsed data...\")\n",
    "    files = read_docs(data_dir = DATA_DIR_2)\n",
    "    data_images = input_dir + \"/images\"\n",
    "    # Load text_node_with_context\n",
    "    with open(os.path.join(input_dir, 'text_node_with_context.pkl'), 'rb') as f:\n",
    "        text_node_with_context = pickle.load(f)\n",
    "    # Load tokenized_documents\n",
    "    with open(os.path.join(input_dir, 'tokenized_documents.pkl'), 'rb') as f:\n",
    "        tokenized_documents = pickle.load(f)\n",
    "    #create retriever for the vector database\n",
    "    ctx = StorageContext.from_defaults(persist_dir=input_dir)\n",
    "    index = load_index_from_storage(ctx)\n",
    "    # Reconstruct bm25\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "    print(\"Data loaded from existing files.\")\n",
    "\n",
    "# Define the QA prompt template\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Below we give parsed text from documents in two different formats, as well as the image.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query. Generate the answer by analyzing parsed markdown, raw text and the related\n",
    "image. Especially, carefully analyze the images to look for the required information.\n",
    "Format the answer in proper format as deems suitable (bulleted lists, sections/sub-sections, tables, etc.).\n",
    "If the query comprises multiple questions, answer each question separately. \n",
    "Give the numbers of all pages and the names of all documents from where you synthesized the response based on the Context.\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(RAG_PROMPT)\n",
    "\n",
    "# Initialize the multimodal LLM\n",
    "MM_LLM = OpenAIMultiModal(model=\"gpt-4o-mini\", temperature=0.0, max_tokens=1024)\n",
    "\n",
    "# Define the QueryEngine integrating all methods\n",
    "class QueryEngine(CustomQueryEngine):\n",
    "    # public fields\n",
    "    qa_prompt: PromptTemplate\n",
    "    multi_modal_llm: OpenAIMultiModal\n",
    "    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None\n",
    "    # private attributes using PrivateAttr\n",
    "    _bm25: BM25Okapi = PrivateAttr()\n",
    "    _llm: OpenAI = PrivateAttr()\n",
    "    _text_node_with_context: List[TextNode] = PrivateAttr()\n",
    "    _vector_index: VectorStoreIndex = PrivateAttr()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        qa_prompt: PromptTemplate,\n",
    "        bm25: BM25Okapi,\n",
    "        multi_modal_llm: OpenAIMultiModal,\n",
    "        vector_index: VectorStoreIndex,\n",
    "        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n",
    "        llm: OpenAI = None,\n",
    "        text_node_with_context: List[TextNode] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            qa_prompt=qa_prompt,\n",
    "            retriever=None,\n",
    "            multi_modal_llm=multi_modal_llm,\n",
    "            node_postprocessors=node_postprocessors\n",
    "        )\n",
    "        self._bm25 = bm25\n",
    "        self._llm = llm\n",
    "        self._text_node_with_context = text_node_with_context\n",
    "        self._vector_index = vector_index  \n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        # Prepare the query bundle\n",
    "        query_bundle = QueryBundle(query_str)\n",
    "\n",
    "        if best_match_25 == 1: #if bm25 search is selected\n",
    "        # Retrieve nodes using BM25\n",
    "            query_tokens = query_str.split()\n",
    "            bm25_scores = self._bm25.get_scores(query_tokens)\n",
    "            top_n_bm25 = 10  # Adjust the number of top nodes to retrieve\n",
    "            top_indices_bm25 = bm25_scores.argsort()[-top_n_bm25:][::-1]\n",
    "            bm25_nodes = [self._text_node_with_context[i] for i in top_indices_bm25]\n",
    "        else:\n",
    "            bm25_nodes = []\n",
    "            print(\"BM25 not selected.\")\n",
    "    \n",
    "        # Retrieve nodes using vector-based retrieval from the vector store\n",
    "        vector_retriever = self._vector_index.as_query_engine().retriever\n",
    "        vector_nodes_with_scores = vector_retriever.retrieve(query_bundle)\n",
    "        vector_nodes = [node.node for node in vector_nodes_with_scores]\n",
    "    \n",
    "        # Combine nodes from both BM25 and vector-based retrieval\n",
    "        all_nodes = vector_nodes + bm25_nodes\n",
    "\n",
    "        if re_ranking == 1: ##if re-ranking is selected\n",
    "            # Apply Cohere Re-ranking to rerank the combined results\n",
    "            documents = [node.text for node in all_nodes]\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    reranked = cohere_client.rerank(\n",
    "                        model=\"rerank-english-v2.0\",\n",
    "                        query=query_str,\n",
    "                        documents=documents,\n",
    "                        top_n=3 #top-3 re-ranked nodes\n",
    "                    )\n",
    "                    break\n",
    "                except cohere.CohereError as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        logging.warning(f\"Error occurred: {str(e)}. Waiting for 60 seconds before retry {attempt + 1}/{max_retries}\")\n",
    "                        time.sleep(60)  # Wait before retrying\n",
    "                    else:\n",
    "                        logging.error(\"Error occurred. Max retries reached. Proceeding without re-ranking.\")\n",
    "                        reranked = None\n",
    "                        break\n",
    "        \n",
    "            if reranked:\n",
    "                reranked_indices = [result.index for result in reranked.results]\n",
    "                nodes = [all_nodes[i] for i in reranked_indices]\n",
    "            else:\n",
    "                nodes = all_nodes[:top_n]  \n",
    "        else:\n",
    "            print(\"Re-ranking not selected.\")\n",
    "            nodes = all_nodes\n",
    "    \n",
    "        # Create image nodes from the images associated with the nodes\n",
    "        image_nodes = [\n",
    "            NodeWithScore(node=ImageNode(image_path=n.metadata[\"image_path\"]))\n",
    "            for n in nodes\n",
    "        ]\n",
    "    \n",
    "        # Create context string from parsed markdown text\n",
    "        ctx_str = \"\\n\\n\".join(\n",
    "            [n.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]\n",
    "        )\n",
    "    \n",
    "        # Prompt for the LLM\n",
    "        fmt_prompt = self.qa_prompt.format(context_str=ctx_str, query_str=query_str)\n",
    "    \n",
    "        # Use the multimodal LLM to interpret images and generate a response\n",
    "        llm_response = self.multi_modal_llm.complete(\n",
    "            prompt=fmt_prompt,\n",
    "            image_documents=[image_node.node for image_node in image_nodes],\n",
    "        )\n",
    "    \n",
    "        # Return the final response\n",
    "        return Response(\n",
    "            response=str(llm_response),\n",
    "            source_nodes=nodes,\n",
    "            metadata={\"text_node_with_context\": self._text_node_with_context, \"image_nodes\": image_nodes},\n",
    "        )\n",
    "\n",
    "    def hyde(self, query: str) -> str: #This hypothetical document embedding method can be further improved and tested\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant helping to improve search results. Expand the following query with additional relevant concepts, synonyms, and terms\n",
    "        for better search results.\n",
    "\n",
    "        Original Query: {query}\n",
    "            \n",
    "        Expanded Query:\n",
    "        \"\"\"\n",
    "        response = self._llm.complete(prompt)\n",
    "        expanded_query = response.text.strip()\n",
    "        return expanded_query\n",
    "\n",
    "# Initialize the query engine with BM25, Cohere Re-ranking, and Query Expansion\n",
    "query_engine = QueryEngine(\n",
    "    qa_prompt=PROMPT,\n",
    "    bm25=bm25,\n",
    "    multi_modal_llm=MM_LLM,\n",
    "    vector_index=index, \n",
    "    node_postprocessors=[],\n",
    "    llm=llm,\n",
    "    text_node_with_context=text_node_with_context)  \n",
    "print(\"All done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "94319224-9333-4ddc-95ae-183ad0e4d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 22:56:49,320 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 22:56:49,825 - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 22:56:57,938 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Most Popular Choices of Companies for Large Language Models (LLMs)\n",
       "\n",
       "1. **Model Preference**:\n",
       "   - **Proprietary Models**: Approximately 62% of workloads are built using proprietary models (e.g., GPT-4).\n",
       "   - **Open Source Models**: About 38% of models used are open source.\n",
       "\n",
       "2. **Procurement Methods**:\n",
       "   - **Indirectly through Cloud Service Providers**: 71% of organizations procure LLMs this way (e.g., Azure, AWS, GCP).\n",
       "   - **Directly through Model Providers**: 68% source models directly from providers (e.g., OpenAI, Anthropic).\n",
       "   - **Indirectly through Model Hubs**: 23% use model hubs (e.g., Hugging Face).\n",
       "\n",
       "### Most Important Factor for Selecting an LLM\n",
       "\n",
       "- **Performance**: \n",
       "  - **Percentage of Respondents Ranking as Top Factor**: 40%\n",
       "  \n",
       "- **Other Factors**:\n",
       "  - Security: 21%\n",
       "  - Customizability: 15%\n",
       "  - Control: 12%\n",
       "  - Cost: 12%\n",
       "\n",
       "### Summary\n",
       "\n",
       "- Companies predominantly favor proprietary models for their LLM needs, with performance being the most critical factor in their selection process.\n",
       "\n",
       "### Source Information\n",
       "\n",
       "- **Pages**: 25, 27\n",
       "- **Document**: AI Report 2 (Perspectives from the ICONIQ Growth GenAI Survey, June 2024)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_query = \"\"\"What are the most popular choices of companies for large language models (LLMs), and which factor \n",
    "is considered to be the most important for selecting an LLM?\"\"\"\n",
    "response = query_engine.query(original_query)\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51305477-8b65-484a-9bfa-71293a09b364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a2b809-5028-4abf-84ed-6a30930bda0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting text nodes...\n",
      "Indexing...\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Simple RAG\"\"\"\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "import os\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up chunking parameters\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 400\n",
    "\n",
    "QA_PROMPT_TMPL = PromptTemplate(template=\"\"\"\\\n",
    "Below we give context from documents.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "You are a helpful assistant tasked with answering questions based solely on the provided information. \n",
    "Given the context information and not prior knowledge, answer the query. \n",
    "If the answer cannot be found directly in the provided context, respond with 'The information is not available in the provided context.'\n",
    "Query: {query_str}\n",
    "Answer: \"\"\")\n",
    "\n",
    "# Load documents and apply chunking\n",
    "files = SimpleDirectoryReader(\"files2\").load_data()\n",
    "simple_parser = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "# Create the vector store index using the text nodes\n",
    "embd_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "llm_gpt = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Global settings\n",
    "Settings.llm = llm_gpt\n",
    "Settings.embed_model = embd_model\n",
    "\n",
    "print(\"Getting text nodes...\")\n",
    "txt_nodes = simple_parser.get_nodes_from_documents(files)\n",
    "print(\"Indexing...\")\n",
    "index_simple = VectorStoreIndex(txt_nodes, embed_model=embd_model)\n",
    "retriever_simple = index_simple.as_retriever()\n",
    "\n",
    "class RAGStringQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"RAG String Query Engine.\"\"\"\n",
    "    \n",
    "    retriever: BaseRetriever\n",
    "    llm: OpenAI\n",
    "    qa_prompt: PromptTemplate  \n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        txt_nodes = self.retriever.retrieve(query_str)\n",
    "        context_str = \"\\n\\n\".join([n.node.get_content() for n in txt_nodes])\n",
    "        \n",
    "        response = self.llm.complete(\n",
    "            self.qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        )\n",
    "        \n",
    "        return str(response)\n",
    "\n",
    "query_engine = RAGStringQueryEngine(\n",
    "    retriever=retriever_simple,\n",
    "    llm=llm_gpt,\n",
    "    qa_prompt=QA_PROMPT_TMPL,\n",
    ")\n",
    "\n",
    "print(\"All done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ec8c3b-056f-4373-a05f-d04fbc3886dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The most popular choices of companies for large language models (LLMs) are proprietary models, with an average of 62% of models being proprietary compared to 38% being open source. The most important factor for selecting an LLM is performance, which is prioritized by 40% of respondents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_query = \"\"\"What are the most popular choices of companies for large language models (LLMs), and which factor \n",
    "is considered to be the most important for selecting an LLM?\"\"\"\n",
    "response = query_engine.query(original_query)\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06575d1b-baf8-4abf-848b-108327711313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
