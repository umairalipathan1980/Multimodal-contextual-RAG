{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a680f94-448a-4d84-89ee-dcd9a7b1919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\h02317\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the parsed directory... immigrant_stats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parsed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 23:08:38,126 - INFO - Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from existing files.\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Multimodal, contextual RAG with BM25+re-ranking\"\"\"\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.core.schema import NodeWithScore, MetadataMode, QueryBundle\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.schema import ImageNode\n",
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from IPython.display import display, Markdown\n",
    "import cohere  \n",
    "import time   \n",
    "from rank_bm25 import BM25Okapi  \n",
    "import logging  \n",
    "from pydantic import PrivateAttr\n",
    "import nest_asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from llama_parse import LlamaParse\n",
    "import re\n",
    "from pathlib import Path\n",
    "import typing as t\n",
    "from typing import Any, List, Optional, Tuple\n",
    "from llama_index.core.schema import TextNode\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate\n",
    "from pydantic import PrivateAttr\n",
    "from llama_index.core import Settings\n",
    "import pickle\n",
    "\n",
    "load_dotenv()\n",
    "best_match_25 = 1\n",
    "re_ranking = 1\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    Settings,\n",
    ")\n",
    "nest_asyncio.apply()\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize Cohere client \n",
    "cohere_client = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "Settings.chunk_size = 800\n",
    "Settings.chunk_overlap = 400\n",
    "\n",
    "# Initialize LlamaParse wit vendor multimodal model\n",
    "# parser = LlamaParse(\n",
    "#     result_type=\"markdown\",\n",
    "#     parsing_instruction=\"You are given a document. Extract the text, tables, and images. Describe the images and summarize the tables in the overall context\",\n",
    "#     use_vendor_multimodal_model=True,\n",
    "#     vendor_multimodal_model_name=\"openai-gpt-4o-mini\",\n",
    "#     vendor_multimodal_api_key=openai.api_key,\n",
    "#     show_progress=True,\n",
    "#     verbose=True,\n",
    "#     invalidate_cache=True,\n",
    "#     do_not_cache=True,\n",
    "#     num_workers=8,\n",
    "#     language=\"en\",\n",
    "#     premium_mode=True,\n",
    "#     api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\")  # Replace with your LLAMA_CLOUD_API_KEY\n",
    "# )\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    premium_mode=True,\n",
    "    api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    ")\n",
    "\n",
    "# Define data directory\n",
    "DATA_DIR = \"files2\" #directory to read files from\n",
    "DATA_DIR_2 = \"parsed\" #directory to save parsing results\n",
    "\n",
    "# Function to read all files from a specified directory\n",
    "def read_docs(data_dir) -> List[str]:\n",
    "    files = []\n",
    "    for f in os.listdir(data_dir):\n",
    "        fname = os.path.join(data_dir, f)\n",
    "        if os.path.isfile(fname):\n",
    "            files.append(fname)\n",
    "    return files\n",
    "\n",
    "# Function to get page number of images using regex on file names\n",
    "def get_img_page_number(file_name):\n",
    "    match = re.search(r\"-page-(\\d+)\\.jpg$\", str(file_name))\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0\n",
    "\n",
    "# Function to get image files sorted by page\n",
    "def _get_sorted_image_files(image_dir):\n",
    "    raw_files = [f for f in list(Path(image_dir).iterdir()) if f.is_file()]\n",
    "    sorted_files = sorted(raw_files, key=get_img_page_number)\n",
    "    return sorted_files\n",
    "\n",
    "# Context prompt template for contextual chunking\n",
    "CONTEXT_PROMPT_TMPL = \"\"\"\n",
    "You are an AI assistant specializing in document analysis. Your task is to provide brief, relevant context for a chunk of text from the given document.\n",
    "Here is the document:\n",
    "<document>\n",
    "{document}\n",
    "</document>\n",
    "\n",
    "Here is the chunk we want to situate within the whole document:\n",
    "<chunk>\n",
    "{chunk}\n",
    "</chunk>\n",
    "\n",
    "Provide a concise context (2-3 sentences) for this chunk, considering the following guidelines:\n",
    "1. Identify the main topic or concept discussed in the chunk.\n",
    "2. Mention any relevant information or comparisons from the broader document context.\n",
    "3. If applicable, note how this information relates to the overall theme or purpose of the document.\n",
    "4. Include any key figures, dates, or percentages that provide important context.\n",
    "5. Do not use phrases like \"This chunk discusses\" or \"This section provides\". Instead, directly state the context.\n",
    "\n",
    "Please give a short succinct context to situate this chunk within the overall document to improve search retrieval of the chunk. \n",
    "Answer only with the succinct context and nothing else.\n",
    "\n",
    "Context:\n",
    "\"\"\"\n",
    "\n",
    "CONTEXT_PROMPT = PromptTemplate(CONTEXT_PROMPT_TMPL)\n",
    "\n",
    "# Function to generate context for each chunk\n",
    "def _assign_context(document: str, chunk: str, llm) -> str:\n",
    "    prompt = CONTEXT_PROMPT.format(document=document, chunk=chunk)\n",
    "    response = llm.complete(prompt)\n",
    "    context = response.text.strip()\n",
    "    return context\n",
    "\n",
    "# Function to create text nodes with context\n",
    "def retrieve_nodes(json_results, image_dir, llm) -> List[TextNode]:\n",
    "    nodes = []\n",
    "    for result in json_results:\n",
    "        json_dicts = result[\"pages\"]\n",
    "        document_name = result[\"file_path\"].split('/')[-1]\n",
    "        docs = [doc[\"md\"] for doc in json_dicts]  # Extract text\n",
    "        image_files = _get_sorted_image_files(image_dir)  # Extract images\n",
    "        # Join all docs to create the full document text\n",
    "        document_text = \"\\n\\n\".join(docs)\n",
    "        for idx, doc in enumerate(docs):\n",
    "            # Generate context for each chunk (page)\n",
    "            context = _assign_context(document_text, doc, llm)\n",
    "            # Combine context with the original chunk\n",
    "            contextualized_content = f\"{context}\\n\\n{doc}\"\n",
    "            # Create the text node with the contextualized content\n",
    "            chunk_metadata = {\"page_num\": idx + 1}\n",
    "            chunk_metadata[\"image_path\"] = str(image_files[idx])\n",
    "            chunk_metadata[\"parsed_text_markdown\"] = docs[idx]\n",
    "        \n",
    "            node = TextNode(\n",
    "                text=contextualized_content,\n",
    "                metadata=chunk_metadata,\n",
    "            )\n",
    "            nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0, max_tokens=16000)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Set LlamaIndex settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Set the PARSING variable\n",
    "PARSING = 0  # Set to 1 to perform parsing, 0 to load existing data\n",
    "\n",
    "# Main code block\n",
    "if PARSING == 1: ##If parsing of a new document is done\n",
    "    files = read_docs(data_dir = DATA_DIR)\n",
    "    dir_name = input(\"Write the name of the directory where parsing results will be stored\")\n",
    "    output_dir = os.path.join(DATA_DIR_2, dir_name)  # Define the new directory path\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)  # Create the directory\n",
    "    image_dir = output_dir + \"/images\"\n",
    "    os.makedirs(image_dir)\n",
    "    print(\"Parsing...\")\n",
    "    json_results = parser.get_json_result(files)\n",
    "    print(\"Getting image dictionaries...\")\n",
    "    images = parser.get_images(json_results, download_path=image_dir)\n",
    "    print(\"Retrieving nodes...\")\n",
    "    # Get text nodes\n",
    "    text_node_with_context = retrieve_nodes(json_results, image_dir, llm)\n",
    "    # Create the vector store index\n",
    "    index = VectorStoreIndex(text_node_with_context, embed_model=embed_model)\n",
    "    index.storage_context.persist(persist_dir=output_dir)\n",
    "    # Build BM25 index\n",
    "    documents = [node.text for node in text_node_with_context]\n",
    "    tokenized_documents = [doc.split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "    # Save bm25 and text_node_with_context\n",
    "    with open(os.path.join(output_dir, 'tokenized_documents.pkl'), 'wb') as f:\n",
    "        pickle.dump(tokenized_documents, f)\n",
    "    with open(os.path.join(output_dir, 'text_node_with_context.pkl'), 'wb') as f:\n",
    "        pickle.dump(text_node_with_context, f)\n",
    "    print(\"Parsing and indexing completed.\")\n",
    "else: ##if the parsed files are to be loaded\n",
    "    input_dir = input(\"Enter the name of the parsed directory...\")\n",
    "    input_dir = DATA_DIR_2 + \"/\"+ input_dir\n",
    "    print(\"Loading parsed data...\")\n",
    "    files = read_docs(data_dir = DATA_DIR_2)\n",
    "    data_images = input_dir + \"/images\"\n",
    "    # Load text_node_with_context\n",
    "    with open(os.path.join(input_dir, 'text_node_with_context.pkl'), 'rb') as f:\n",
    "        text_node_with_context = pickle.load(f)\n",
    "    # Load tokenized_documents\n",
    "    with open(os.path.join(input_dir, 'tokenized_documents.pkl'), 'rb') as f:\n",
    "        tokenized_documents = pickle.load(f)\n",
    "    #create retriever for the vector database\n",
    "    ctx = StorageContext.from_defaults(persist_dir=input_dir)\n",
    "    index = load_index_from_storage(ctx)\n",
    "    # Reconstruct bm25\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "    print(\"Data loaded from existing files.\")\n",
    "\n",
    "# Define the QA prompt template\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Below we give parsed text from documents in two different formats, as well as the image.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query in a precise and concise way. Do not provide unnecessary details.\n",
    "Provide brief insights wherever necessary.\n",
    "Generate the answer by analyzing parsed markdown, raw text and the related image. \n",
    "Especially, carefully analyze the images to look for the required information.\n",
    "Format the answer in proper format as deems suitable (bulleted lists, sections/sub-sections, tables, etc.).\n",
    "If the query comprises multiple questions, answer each question separately. \n",
    "Give the numbers of all related pages and the names of all related documents from where you synthesized the response based on the Context.\n",
    "Only include those page number and document names which were used to generate the final response, not necessarly all which were handed over to you.\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(RAG_PROMPT)\n",
    "\n",
    "# Initialize the multimodal LLM\n",
    "MM_LLM = OpenAIMultiModal(model=\"gpt-4o-mini\", temperature=0.0, max_tokens=16000)\n",
    "\n",
    "class QueryEngine(CustomQueryEngine):\n",
    "    # Public fields\n",
    "    qa_prompt: PromptTemplate\n",
    "    multi_modal_llm: OpenAIMultiModal\n",
    "    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None\n",
    "\n",
    "    # Private attributes using PrivateAttr\n",
    "    _bm25: BM25Okapi = PrivateAttr()\n",
    "    _llm: OpenAI = PrivateAttr()\n",
    "    _text_node_with_context: List[TextNode] = PrivateAttr()\n",
    "    _vector_index: VectorStoreIndex = PrivateAttr()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        qa_prompt: PromptTemplate,\n",
    "        bm25: BM25Okapi,\n",
    "        multi_modal_llm: OpenAIMultiModal,\n",
    "        vector_index: VectorStoreIndex,\n",
    "        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n",
    "        llm: OpenAI = None,\n",
    "        text_node_with_context: List[TextNode] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            qa_prompt=qa_prompt,\n",
    "            retriever=None,\n",
    "            multi_modal_llm=multi_modal_llm,\n",
    "            node_postprocessors=node_postprocessors\n",
    "        )\n",
    "        self._bm25 = bm25\n",
    "        self._llm = llm\n",
    "        self._text_node_with_context = text_node_with_context\n",
    "        self._vector_index = vector_index\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        # Prepare the query bundle\n",
    "        query_bundle = QueryBundle(query_str)\n",
    "\n",
    "        bm25_nodes = []\n",
    "        if best_match_25 == 1:  # if BM25 search is selected\n",
    "            # Retrieve nodes using BM25\n",
    "            query_tokens = query_str.split()\n",
    "            bm25_scores = self._bm25.get_scores(query_tokens)\n",
    "            top_n_bm25 = 5  # Adjust the number of top nodes to retrieve\n",
    "            # Get indices of top BM25 scores\n",
    "            top_indices_bm25 = bm25_scores.argsort()[-top_n_bm25:][::-1]\n",
    "            bm25_nodes = [self._text_node_with_context[i] for i in top_indices_bm25]\n",
    "            logging.info(f\"BM25 nodes retrieved: {len(bm25_nodes)}\")\n",
    "        else:\n",
    "            logging.info(\"BM25 not selected.\")\n",
    "\n",
    "        # Retrieve nodes using vector-based retrieval from the vector store\n",
    "        vector_retriever = self._vector_index.as_query_engine().retriever\n",
    "        vector_nodes_with_scores = vector_retriever.retrieve(query_bundle)\n",
    "        # Specify the number of top vectors you want\n",
    "        top_n_vectors = 5  # Adjust this value as needed\n",
    "        # Get only the top 'n' nodes\n",
    "        top_vector_nodes_with_scores = vector_nodes_with_scores[:top_n_vectors]\n",
    "        vector_nodes = [node.node for node in top_vector_nodes_with_scores]\n",
    "        logging.info(f\"Vector nodes retrieved: {len(vector_nodes)}\")\n",
    "\n",
    "        # Combine nodes and remove duplicates\n",
    "        all_nodes = vector_nodes + bm25_nodes\n",
    "        unique_nodes_dict = {node.node_id: node for node in all_nodes}\n",
    "        unique_nodes = list(unique_nodes_dict.values())\n",
    "        logging.info(f\"Unique nodes after deduplication: {len(unique_nodes)}\")\n",
    "\n",
    "        nodes = unique_nodes\n",
    "\n",
    "        if re_ranking == 1:  # if re-ranking is selected\n",
    "            # Apply Cohere Re-ranking to rerank the combined results\n",
    "            documents = [node.get_content() for node in nodes]\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    reranked = cohere_client.rerank(\n",
    "                        model=\"rerank-english-v2.0\",\n",
    "                        query=query_str,\n",
    "                        documents=documents,\n",
    "                        top_n=3  # top-3 re-ranked nodes\n",
    "                    )\n",
    "                    break\n",
    "                except CohereError as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        logging.warning(f\"Error occurred: {str(e)}. Waiting for 60 seconds before retry {attempt + 1}/{max_retries}\")\n",
    "                        time.sleep(60)  # Wait before retrying\n",
    "                    else:\n",
    "                        logging.error(\"Error occurred. Max retries reached. Proceeding without re-ranking.\")\n",
    "                        reranked = None\n",
    "                        break\n",
    "\n",
    "            if reranked:\n",
    "                reranked_indices = [result.index for result in reranked.results]\n",
    "                nodes = [nodes[i] for i in reranked_indices]\n",
    "            else:\n",
    "                nodes = nodes[:3]  # Fallback to top 3 nodes\n",
    "            logging.info(f\"Nodes after re-ranking: {len(nodes)}\")\n",
    "        else:\n",
    "            logging.info(\"Re-ranking not selected.\")\n",
    "\n",
    "        # Limit and filter node content for context string\n",
    "        max_context_length = 16000  # Adjust based on LLM's token limit\n",
    "        current_length = 0\n",
    "        filtered_nodes = []\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        from transformers import GPT2TokenizerFast\n",
    "        tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "        for node in nodes:\n",
    "            content = node.get_content(metadata_mode=MetadataMode.LLM).strip()\n",
    "            node_length = len(tokenizer.encode(content))\n",
    "            logging.info(f\"Node ID: {node.node_id}, Content Length (tokens): {node_length}\")\n",
    "            if not content:\n",
    "                logging.warning(f\"Node ID: {node.node_id} has empty content. Skipping.\")\n",
    "                continue\n",
    "            if current_length + node_length <= max_context_length:\n",
    "                filtered_nodes.append(node)\n",
    "                current_length += node_length\n",
    "            else:\n",
    "                logging.info(f\"Reached max context length with Node ID: {node.node_id}\")\n",
    "                break\n",
    "        logging.info(f\"Filtered nodes for context: {len(filtered_nodes)}\")\n",
    "\n",
    "        # Create context string\n",
    "        ctx_str = \"\\n\\n\".join(\n",
    "            [n.get_content(metadata_mode=MetadataMode.LLM).strip() for n in filtered_nodes]\n",
    "        )\n",
    "\n",
    "        #print(f\"ctx_str:\\n\\n{ctx_str}\")\n",
    "\n",
    "        # Create image nodes from the images associated with the nodes\n",
    "        image_nodes = []\n",
    "        for n in filtered_nodes:\n",
    "            if \"image_path\" in n.metadata:\n",
    "                image_nodes.append(\n",
    "                    NodeWithScore(node=ImageNode(image_path=n.metadata[\"image_path\"]))\n",
    "                )\n",
    "            else:\n",
    "                logging.warning(f\"Node ID: {n.node_id} lacks 'image_path' metadata.\")\n",
    "        logging.info(f\"Image nodes created: {len(image_nodes)}\")\n",
    "\n",
    "        # Prepare prompt for the LLM\n",
    "        fmt_prompt = self.qa_prompt.format(context_str=ctx_str, query_str=query_str)\n",
    "\n",
    "        #print(f\"\\n\\n fmt_prompt:\\n\\n{fmt_prompt}\")\n",
    "\n",
    "        # Use the multimodal LLM to interpret images and generate a response\n",
    "        llm_response = self.multi_modal_llm.complete(\n",
    "            prompt=fmt_prompt,\n",
    "            image_documents=[image_node.node for image_node in image_nodes],\n",
    "            max_tokens=16000\n",
    "        )\n",
    "\n",
    "        logging.info(f\"LLM response generated.\")\n",
    "\n",
    "        # Return the final response\n",
    "        return Response(\n",
    "            response=str(llm_response),\n",
    "            source_nodes=filtered_nodes,\n",
    "            metadata={\n",
    "                \"text_node_with_context\": self._text_node_with_context,\n",
    "                \"image_nodes\": image_nodes,\n",
    "            },\n",
    "        )\n",
    "\n",
    "# Initialize the query engine with BM25, Cohere Re-ranking, and Query Expansion\n",
    "query_engine = QueryEngine(\n",
    "    qa_prompt=PROMPT,\n",
    "    bm25=bm25,\n",
    "    multi_modal_llm=MM_LLM,\n",
    "    vector_index=index,\n",
    "    node_postprocessors=[],\n",
    "    llm=llm,\n",
    "    text_node_with_context=text_node_with_context\n",
    ")\n",
    "print(\"All done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94319224-9333-4ddc-95ae-183ad0e4d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 23:08:40,870 - INFO - BM25 nodes retrieved: 5\n",
      "2024-10-16 23:08:41,787 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 23:08:41,798 - INFO - Vector nodes retrieved: 2\n",
      "2024-10-16 23:08:41,800 - INFO - Unique nodes after deduplication: 6\n",
      "2024-10-16 23:08:42,110 - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 23:08:42,121 - INFO - Nodes after re-ranking: 3\n",
      "C:\\Users\\h02317\\.my_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-10-16 23:08:50,819 - INFO - Node ID: 9a75fda6-5048-4be8-8f3f-c99031e829e8, Content Length (tokens): 719\n",
      "2024-10-16 23:08:50,826 - INFO - Node ID: af38a862-4487-4cea-a975-9b0150f36e76, Content Length (tokens): 928\n",
      "2024-10-16 23:08:50,831 - INFO - Node ID: fce0393a-90c9-48d5-92db-f1783eeeb070, Content Length (tokens): 607\n",
      "2024-10-16 23:08:50,832 - INFO - Filtered nodes for context: 3\n",
      "2024-10-16 23:08:50,833 - INFO - Image nodes created: 3\n",
      "2024-10-16 23:08:58,799 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 23:08:58,815 - INFO - LLM response generated.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Top Countries for First Residence Permits in Finland (2023)\n",
       "\n",
       "The top countries whose citizens received the highest number of first residence permits from the Finnish Immigration Service in 2023 are:\n",
       "\n",
       "| Country       | Number of Permits |\n",
       "|---------------|-------------------|\n",
       "| Philippines   | 2,861             |\n",
       "| Russia        | 1,436             |\n",
       "| India         | 1,159             |\n",
       "| China         | 768               |\n",
       "| Kosovo        | 742               |\n",
       "\n",
       "### Country with the Highest Number of Permits\n",
       "\n",
       "- **Philippines** received the highest number of first residence permits, totaling **2,861**.\n",
       "\n",
       "### Related Pages and Documents\n",
       "- **Pages Used**: 9, 4\n",
       "- **Documents**: Parsed immigrant statistics documents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_query = \"\"\"What are the top countries to whose citizens the Finnish Immigration Service issued the highest number of first residence permits in 2023?\n",
    "Which of these countries received the highest number of first residence permits?\"\"\"\n",
    "response = query_engine.query(original_query)\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51305477-8b65-484a-9bfa-71293a09b364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a2b809-5028-4abf-84ed-6a30930bda0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting text nodes...\n",
      "Indexing...\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Simple RAG\"\"\"\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "import os\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up chunking parameters\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 400\n",
    "\n",
    "QA_PROMPT_TMPL = PromptTemplate(template=\"\"\"\\\n",
    "Below we give context from documents.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "You are a helpful assistant tasked with answering questions based solely on the provided information. \n",
    "Given the context information and not prior knowledge, answer the query. \n",
    "If the answer cannot be found directly in the provided context, respond with 'The information is not available in the provided context.'\n",
    "Query: {query_str}\n",
    "Answer: \"\"\")\n",
    "\n",
    "# Load documents and apply chunking\n",
    "files = SimpleDirectoryReader(\"files2\").load_data()\n",
    "simple_parser = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "# Create the vector store index using the text nodes\n",
    "embd_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "llm_gpt = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Global settings\n",
    "Settings.llm = llm_gpt\n",
    "Settings.embed_model = embd_model\n",
    "\n",
    "print(\"Getting text nodes...\")\n",
    "txt_nodes = simple_parser.get_nodes_from_documents(files)\n",
    "print(\"Indexing...\")\n",
    "index_simple = VectorStoreIndex(txt_nodes, embed_model=embd_model)\n",
    "retriever_simple = index_simple.as_retriever()\n",
    "\n",
    "class RAGStringQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"RAG String Query Engine.\"\"\"\n",
    "    \n",
    "    retriever: BaseRetriever\n",
    "    llm: OpenAI\n",
    "    qa_prompt: PromptTemplate  \n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        txt_nodes = self.retriever.retrieve(query_str)\n",
    "        context_str = \"\\n\\n\".join([n.node.get_content() for n in txt_nodes])\n",
    "        \n",
    "        response = self.llm.complete(\n",
    "            self.qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        )\n",
    "        \n",
    "        return str(response)\n",
    "\n",
    "query_engine = RAGStringQueryEngine(\n",
    "    retriever=retriever_simple,\n",
    "    llm=llm_gpt,\n",
    "    qa_prompt=QA_PROMPT_TMPL,\n",
    ")\n",
    "\n",
    "print(\"All done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ec8c3b-056f-4373-a05f-d04fbc3886dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The information is not available in the provided context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_query = \"\"\"What are the top countries to whose citizens the Finnish Immigration Service issued the highest number of first residence permits in 2023?\n",
    "Which of these countries received the highest number of first residence permits?\"\"\"\n",
    "response = query_engine.query(original_query)\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06575d1b-baf8-4abf-848b-108327711313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
